{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransE的pytorch实现\n",
    "\n",
    "TransE不是神经网络模型，它无法理解句子的含义，因而得分不高，随便跑跑mrr大概在42左右，调下参大概有45-50。\n",
    "\n",
    "TransE的优势在于不吃算力，仅靠CPU就能运行，这里我写的GPU版本速度更快。而且代码简单。\n",
    "\n",
    "又试了一下，我已经复现不出自己的得分了，太玄学了，大概策略就是先norm取1得到最优模型，加载该模型并norm改为2训练得到最优模型，再加载该模型norm取3得到最优模型，再反复调整后最优分数就是50左右；\n",
    "在算力允许的情况下，batchsize尽可能大，比如10万-150万（就是全部）；然后就是embedding维度我感觉影响不大，100维包含的信息足够丰富了，试过256和512，貌似大一点有时要好一些，不太确定，太大了会比较费算力；学习率粗调可以在0.01-0.001之间，微调就在0.001-0.0001之间吧。\n",
    "\n",
    "提供给大家学习，代码也可以在我的github下载：https://github.com/renqi1/TransE_Pytorch_OpenBG500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集和验证集\n",
    "class TripleDataset(data.Dataset):\n",
    "    def __init__(self, ent2id, rel2id, triple_data_list):\n",
    "        self.ent2id = ent2id\n",
    "        self.rel2id = rel2id\n",
    "        self.data = triple_data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        head, relation, tail = self.data[index]\n",
    "        head_id = self.ent2id[head]\n",
    "        relation_id = self.rel2id[relation]\n",
    "        tail_id = self.ent2id[tail]\n",
    "        return head_id, relation_id, tail_id\n",
    "\n",
    "# 测试集    \n",
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, ent2id, rel2id, test_data_list):\n",
    "        self.ent2id = ent2id\n",
    "        self.rel2id = rel2id\n",
    "        self.data = test_data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        head, relation = self.data[index]\n",
    "        head_id = self.ent2id[head]\n",
    "        relation_id = self.rel2id[relation]\n",
    "        return head_id, relation_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransE模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransE(nn.Module):\n",
    "\n",
    "    def __init__(self, entity_num, relation_num, norm=1, dim=100):\n",
    "        super(TransE, self).__init__()\n",
    "        self.norm = norm\n",
    "        self.dim = dim\n",
    "        self.entity_num = entity_num\n",
    "        self.entities_emb = self._init_emb(entity_num)\n",
    "        self.relations_emb = self._init_emb(relation_num)\n",
    "\n",
    "    def _init_emb(self, num_embeddings):\n",
    "        embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=self.dim)\n",
    "        uniform_range = 6 / np.sqrt(self.dim)\n",
    "        embedding.weight.data.uniform_(-uniform_range, uniform_range)\n",
    "        embedding.weight.data = torch.div(embedding.weight.data, embedding.weight.data.norm(p=2, dim=1, keepdim=True))\n",
    "        return embedding\n",
    "\n",
    "    def forward(self, positive_triplets: torch.LongTensor, negative_triplets: torch.LongTensor):\n",
    "        positive_distances = self._distance(positive_triplets)\n",
    "        negative_distances = self._distance(negative_triplets)\n",
    "        return positive_distances, negative_distances\n",
    "\n",
    "    def _distance(self, triplets):\n",
    "        heads = self.entities_emb(triplets[:, 0])\n",
    "        relations = self.relations_emb(triplets[:, 1])\n",
    "        tails = self.entities_emb(triplets[:, 2])\n",
    "        return (heads + relations - tails).norm(p=self.norm, dim=1)\n",
    "\n",
    "    def link_predict(self, head, relation, tail=None, k=10):\n",
    "        # h_add_r: [batch size, embed size] -> [batch size, 1, embed size] -> [batch size, entity num, embed size]\n",
    "        h_add_r = self.entities_emb(head) + self.relations_emb(relation)\n",
    "        h_add_r = torch.unsqueeze(h_add_r, dim=1)\n",
    "        h_add_r = h_add_r.expand(h_add_r.shape[0], self.entity_num, self.dim)\n",
    "        # embed_tail: [batch size, embed size] -> [batch size, entity num, embed size]\n",
    "        embed_tail = self.entities_emb.weight.data.expand(h_add_r.shape[0], self.entity_num, self.dim)\n",
    "        # values: [batch size, k] scores, the smaller, the better\n",
    "        # indices: [batch size, k] indices of entities ranked by scores\n",
    "        values, indices = torch.topk(torch.norm(h_add_r - embed_tail, dim=2), k=self.entity_num, dim=1, largest=False)\n",
    "        if tail is not None:\n",
    "            tail = tail.view(-1, 1)\n",
    "            rank_num = torch.eq(indices, tail).nonzero().permute(1, 0)[1]+1\n",
    "            rank_num[rank_num > 9] = 10000\n",
    "            mrr = torch.sum(1/rank_num)\n",
    "            hits_1_num = torch.sum(torch.eq(indices[:, :1], tail)).item()\n",
    "            hits_3_num = torch.sum(torch.eq(indices[:, :3], tail)).item()\n",
    "            hits_10_num = torch.sum(torch.eq(indices[:, :10], tail)).item()\n",
    "            return mrr, hits_1_num, hits_3_num, hits_10_num     # 返回一个batchsize, mrr的和，hit@k的和\n",
    "        return indices[:, :k]\n",
    "\n",
    "    def evaluate(self, data_loader, dev_num=5000.0):\n",
    "        mrr_sum = hits_1_nums = hits_3_nums = hits_10_nums = 0\n",
    "        for heads, relations, tails in tqdm.tqdm(data_loader):\n",
    "            mrr_sum_batch, hits_1_num, hits_3_num, hits_10_num = self.link_predict(heads.cuda(), relations.cuda(), tails.cuda())\n",
    "            mrr_sum += mrr_sum_batch\n",
    "            hits_1_nums += hits_1_num\n",
    "            hits_3_nums += hits_3_num\n",
    "            hits_10_nums += hits_10_num\n",
    "        return mrr_sum/dev_num, hits_1_nums/dev_num, hits_3_nums/dev_num, hits_10_nums/dev_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchsize增大，得分略有上升\n",
    "train_batch_size = 100000\n",
    "dev_batch_size = 20  # 显存不够就调小\n",
    "test_batch_size = 20\n",
    "epochs = 40\n",
    "margin = 1\n",
    "print_frequency = 5  # 每多少step输出一次信息\n",
    "validation = True  # 是否验证，验证比较费时\n",
    "dev_interval = 5  # 每多少轮验证一次，微调设小一点，会保存最佳权重\n",
    "best_mrr = 0\n",
    "learning_rate = 0.001  # 学习率建议粗调0.01-0.001，精调0.001-0.0001\n",
    "distance_norm = 3  # 论文是L1距离效果不好，取2或3效果好\n",
    "embedding_dim = 100  # 维度增大可能会有提升，我感觉没用，100维包含的信息足够丰富"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('OpenBG500/OpenBG500_entity2text.tsv', 'r', encoding='utf-8') as fp:\n",
    "    dat = fp.readlines()\n",
    "    lines = [line.strip('\\n').split('\\t') for line in dat]\n",
    "ent2id = {line[0]: i for i, line in enumerate(lines)}\n",
    "id2ent = {i: line[0] for i, line in enumerate(lines)}\n",
    "with open('OpenBG500/OpenBG500_relation2text.tsv', 'r', encoding='utf-8') as fp:\n",
    "    dat = fp.readlines()\n",
    "    lines = [line.strip().split('\\t') for line in dat]\n",
    "rel2id = {line[0]: i for i, line in enumerate(lines)}\n",
    "with open('OpenBG500/OpenBG500_train.tsv', 'r', encoding='utf-8') as fp:\n",
    "    dat = fp.readlines()\n",
    "    train = [line.strip('\\n').split('\\t') for line in dat]\n",
    "with open('OpenBG500/OpenBG500_dev.tsv', 'r', encoding='utf-8') as fp:\n",
    "    dat = fp.readlines()\n",
    "    dev = [line.strip('\\n').split('\\t') for line in dat]\n",
    "with open('OpenBG500/OpenBG500_test.tsv', 'r', encoding='utf-8') as fp:\n",
    "    test = fp.readlines()\n",
    "    test = [line.strip('\\n').split('\\t') for line in test]\n",
    "# 构建数据集\n",
    "train_dataset = TripleDataset(ent2id, rel2id, train)\n",
    "dev_dataset = TripleDataset(ent2id, rel2id, dev)\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "dev_data_loader = data.DataLoader(dev_dataset, batch_size=dev_batch_size)\n",
    "test_dataset = TestDataset(ent2id, rel2id, test)\n",
    "test_data_loader = data.DataLoader(test_dataset, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练和验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "epoch:0/40, step:0/13, loss=1.0004600286483765, avg_loss=1.0004600286483765\n",
      "epoch:0/40, step:5/13, loss=0.9924210906028748, avg_loss=0.9964226484298706\n",
      "epoch:0/40, step:10/13, loss=0.984215259552002, avg_loss=0.9923558831214905\n",
      "epoch:0/40, all_loss=12.878626823425293\n",
      "epoch:1/40, step:0/13, loss=0.9761244058609009, avg_loss=0.9761244058609009\n",
      "epoch:1/40, step:5/13, loss=0.9668070077896118, avg_loss=0.9715768694877625\n",
      "epoch:1/40, step:10/13, loss=0.9587149620056152, avg_loss=0.9672104716300964\n",
      "epoch:1/40, all_loss=12.5520658493042\n",
      "epoch:2/40, step:0/13, loss=0.9526633620262146, avg_loss=0.9526633620262146\n",
      "epoch:2/40, step:5/13, loss=0.9433406591415405, avg_loss=0.94773268699646\n",
      "epoch:2/40, step:10/13, loss=0.9345195293426514, avg_loss=0.943464994430542\n",
      "epoch:2/40, all_loss=12.2422513961792\n",
      "epoch:3/40, step:0/13, loss=0.9284957647323608, avg_loss=0.9284957647323608\n",
      "epoch:3/40, step:5/13, loss=0.9203886985778809, avg_loss=0.9242886304855347\n",
      "epoch:3/40, step:10/13, loss=0.9115201234817505, avg_loss=0.9200793504714966\n",
      "epoch:3/40, all_loss=11.93942642211914\n",
      "epoch:4/40, step:0/13, loss=0.9055721759796143, avg_loss=0.9055721759796143\n",
      "epoch:4/40, step:5/13, loss=0.8978390693664551, avg_loss=0.9012894034385681\n",
      "epoch:4/40, step:10/13, loss=0.8887841701507568, avg_loss=0.897139847278595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/250 [00:00<00:46,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4/40, all_loss=11.642279624938965\n",
      "testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:06<00:00, 39.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrr: 0.026399999856948853, hit@1: 0.0264, hit@3: 0.386, hit@10: 0.4986  *\n",
      "epoch:5/40, step:0/13, loss=0.8833775520324707, avg_loss=0.8833775520324707\n",
      "epoch:5/40, step:5/13, loss=0.8748413920402527, avg_loss=0.8788034319877625\n",
      "epoch:5/40, step:10/13, loss=0.8668171763420105, avg_loss=0.8747958540916443\n",
      "epoch:5/40, all_loss=11.350102424621582\n",
      "epoch:6/40, step:0/13, loss=0.8607047200202942, avg_loss=0.8607047200202942\n",
      "epoch:6/40, step:5/13, loss=0.8524377942085266, avg_loss=0.8565347194671631\n",
      "epoch:6/40, step:10/13, loss=0.8449050784111023, avg_loss=0.8524928092956543\n",
      "epoch:6/40, all_loss=11.0615234375\n",
      "epoch:7/40, step:0/13, loss=0.8379879593849182, avg_loss=0.8379879593849182\n",
      "epoch:7/40, step:5/13, loss=0.8299741744995117, avg_loss=0.833824872970581\n",
      "epoch:7/40, step:10/13, loss=0.8220023512840271, avg_loss=0.8298749923706055\n",
      "epoch:7/40, all_loss=10.767749786376953\n",
      "epoch:8/40, step:0/13, loss=0.8162221908569336, avg_loss=0.8162221908569336\n",
      "epoch:8/40, step:5/13, loss=0.8068988919258118, avg_loss=0.8115445971488953\n",
      "epoch:8/40, step:10/13, loss=0.7985112071037292, avg_loss=0.8072609901428223\n",
      "epoch:8/40, all_loss=10.472427368164062\n",
      "epoch:9/40, step:0/13, loss=0.7922348380088806, avg_loss=0.7922348380088806\n",
      "epoch:9/40, step:5/13, loss=0.7835355997085571, avg_loss=0.7882490158081055\n",
      "epoch:9/40, step:10/13, loss=0.7756718397140503, avg_loss=0.7839122414588928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/250 [00:00<00:06, 36.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9/40, all_loss=10.168965339660645\n",
      "testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:06<00:00, 40.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrr: 0.31839999556541443, hit@1: 0.3184, hit@3: 0.5502, hit@10: 0.7048  *\n",
      "epoch:10/40, step:0/13, loss=0.7709746956825256, avg_loss=0.7709746956825256\n",
      "epoch:10/40, step:5/13, loss=0.7604309320449829, avg_loss=0.765308141708374\n",
      "epoch:10/40, step:10/13, loss=0.7523741722106934, avg_loss=0.7608752250671387\n",
      "epoch:10/40, all_loss=9.868080139160156\n",
      "epoch:11/40, step:0/13, loss=0.7459456324577332, avg_loss=0.7459456324577332\n",
      "epoch:11/40, step:5/13, loss=0.7371506094932556, avg_loss=0.7412559390068054\n",
      "epoch:11/40, step:10/13, loss=0.7281594276428223, avg_loss=0.7366960644721985\n",
      "epoch:11/40, all_loss=9.551908493041992\n",
      "epoch:12/40, step:0/13, loss=0.7223385572433472, avg_loss=0.7223385572433472\n",
      "epoch:12/40, step:5/13, loss=0.712777316570282, avg_loss=0.7171534895896912\n",
      "epoch:12/40, step:10/13, loss=0.7030938863754272, avg_loss=0.7125251889228821\n",
      "epoch:12/40, all_loss=9.23984432220459\n",
      "epoch:13/40, step:0/13, loss=0.6953514814376831, avg_loss=0.6953514814376831\n",
      "epoch:13/40, step:5/13, loss=0.6860536336898804, avg_loss=0.6915565729141235\n",
      "epoch:13/40, step:10/13, loss=0.677520751953125, avg_loss=0.6874890327453613\n",
      "epoch:13/40, all_loss=8.913288116455078\n",
      "epoch:14/40, step:0/13, loss=0.6720731258392334, avg_loss=0.6720731258392334\n",
      "epoch:14/40, step:5/13, loss=0.6621931791305542, avg_loss=0.6673516035079956\n",
      "epoch:14/40, step:10/13, loss=0.6535441875457764, avg_loss=0.6626569628715515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/250 [00:00<00:06, 36.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14/40, all_loss=8.590277671813965\n",
      "testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:06<00:00, 40.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrr: 0.3951999843120575, hit@1: 0.3952, hit@3: 0.5966, hit@10: 0.7438  *\n",
      "epoch:15/40, step:0/13, loss=0.6451003551483154, avg_loss=0.6451003551483154\n",
      "epoch:15/40, step:5/13, loss=0.6357419490814209, avg_loss=0.641369104385376\n",
      "epoch:15/40, step:10/13, loss=0.6278892159461975, avg_loss=0.6366351246833801\n",
      "epoch:15/40, all_loss=8.252645492553711\n",
      "epoch:16/40, step:0/13, loss=0.6209930181503296, avg_loss=0.6209930181503296\n",
      "epoch:16/40, step:5/13, loss=0.6099186539649963, avg_loss=0.6163473129272461\n",
      "epoch:16/40, step:10/13, loss=0.6017064452171326, avg_loss=0.6112903952598572\n",
      "epoch:16/40, all_loss=7.92219877243042\n",
      "epoch:17/40, step:0/13, loss=0.5952631831169128, avg_loss=0.5952631831169128\n",
      "epoch:17/40, step:5/13, loss=0.5852000713348389, avg_loss=0.5905779004096985\n",
      "epoch:17/40, step:10/13, loss=0.5738667249679565, avg_loss=0.585422158241272\n",
      "epoch:17/40, all_loss=7.585738658905029\n",
      "epoch:18/40, step:0/13, loss=0.5691049695014954, avg_loss=0.5691049695014954\n",
      "epoch:18/40, step:5/13, loss=0.5609580874443054, avg_loss=0.5650838613510132\n",
      "epoch:18/40, step:10/13, loss=0.5525952577590942, avg_loss=0.5606524348258972\n",
      "epoch:18/40, all_loss=7.265087604522705\n",
      "epoch:19/40, step:0/13, loss=0.5444956421852112, avg_loss=0.5444956421852112\n",
      "epoch:19/40, step:5/13, loss=0.5350615978240967, avg_loss=0.5398629307746887\n",
      "epoch:19/40, step:10/13, loss=0.5278856158256531, avg_loss=0.5360151529312134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5/250 [00:00<00:06, 40.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19/40, all_loss=6.947169303894043\n",
      "testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:06<00:00, 39.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrr: 0.4195999801158905, hit@1: 0.4196, hit@3: 0.613, hit@10: 0.7562  *\n",
      "epoch:20/40, step:0/13, loss=0.5202771425247192, avg_loss=0.5202771425247192\n",
      "epoch:20/40, step:5/13, loss=0.5125833749771118, avg_loss=0.5165330767631531\n",
      "epoch:20/40, step:10/13, loss=0.5044776797294617, avg_loss=0.51270592212677\n",
      "epoch:20/40, all_loss=6.646373271942139\n",
      "epoch:21/40, step:0/13, loss=0.4988667964935303, avg_loss=0.4988667964935303\n",
      "epoch:21/40, step:5/13, loss=0.49106523394584656, avg_loss=0.49422797560691833\n",
      "epoch:21/40, step:10/13, loss=0.4829302132129669, avg_loss=0.4908514618873596\n",
      "epoch:21/40, all_loss=6.362898349761963\n",
      "epoch:22/40, step:0/13, loss=0.47766733169555664, avg_loss=0.47766733169555664\n",
      "epoch:22/40, step:5/13, loss=0.4700329303741455, avg_loss=0.4742520749568939\n",
      "epoch:22/40, step:10/13, loss=0.4629727900028229, avg_loss=0.47101855278015137\n",
      "epoch:22/40, all_loss=6.105311393737793\n",
      "epoch:23/40, step:0/13, loss=0.4578325152397156, avg_loss=0.4578325152397156\n",
      "epoch:23/40, step:5/13, loss=0.4510737359523773, avg_loss=0.4552048146724701\n",
      "epoch:23/40, step:10/13, loss=0.44611749053001404, avg_loss=0.45209798216819763\n",
      "epoch:23/40, all_loss=5.862863063812256\n",
      "epoch:24/40, step:0/13, loss=0.4432310163974762, avg_loss=0.4432310163974762\n",
      "epoch:24/40, step:5/13, loss=0.4340769052505493, avg_loss=0.4377228617668152\n",
      "epoch:24/40, step:10/13, loss=0.4274272918701172, avg_loss=0.4347665309906006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/250 [00:00<00:06, 36.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24/40, all_loss=5.637955188751221\n",
      "testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:06<00:00, 40.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrr: 0.42980000376701355, hit@1: 0.4298, hit@3: 0.6226, hit@10: 0.7646  *\n",
      "epoch:25/40, step:0/13, loss=0.4240056276321411, avg_loss=0.4240056276321411\n",
      "epoch:25/40, step:5/13, loss=0.4187479615211487, avg_loss=0.4217613935470581\n",
      "epoch:25/40, step:10/13, loss=0.4119781255722046, avg_loss=0.41844654083251953\n",
      "epoch:25/40, all_loss=5.423857688903809\n",
      "epoch:26/40, step:0/13, loss=0.40720611810684204, avg_loss=0.40720611810684204\n",
      "epoch:26/40, step:5/13, loss=0.40435922145843506, avg_loss=0.40535134077072144\n",
      "epoch:26/40, step:10/13, loss=0.3975265622138977, avg_loss=0.40281590819358826\n",
      "epoch:26/40, all_loss=5.228335857391357\n",
      "epoch:27/40, step:0/13, loss=0.3926262855529785, avg_loss=0.3926262855529785\n",
      "epoch:27/40, step:5/13, loss=0.38815367221832275, avg_loss=0.39130398631095886\n",
      "epoch:27/40, step:10/13, loss=0.3843480348587036, avg_loss=0.38882681727409363\n",
      "epoch:27/40, all_loss=5.042997360229492\n",
      "epoch:28/40, step:0/13, loss=0.3785274922847748, avg_loss=0.3785274922847748\n",
      "epoch:28/40, step:5/13, loss=0.3741745948791504, avg_loss=0.3771188259124756\n",
      "epoch:28/40, step:10/13, loss=0.3702292740345001, avg_loss=0.374830424785614\n",
      "epoch:28/40, all_loss=4.860633850097656\n",
      "epoch:29/40, step:0/13, loss=0.36565589904785156, avg_loss=0.36565589904785156\n",
      "epoch:29/40, step:5/13, loss=0.3608603775501251, avg_loss=0.3639713525772095\n",
      "epoch:29/40, step:10/13, loss=0.3589763045310974, avg_loss=0.3621947765350342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/250 [00:00<00:06, 36.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29/40, all_loss=4.700049877166748\n",
      "testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:06<00:00, 40.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrr: 0.43199998140335083, hit@1: 0.432, hit@3: 0.6212, hit@10: 0.7682  *\n",
      "epoch:30/40, step:0/13, loss=0.3555942177772522, avg_loss=0.3555942177772522\n",
      "epoch:30/40, step:5/13, loss=0.351566344499588, avg_loss=0.35308966040611267\n",
      "epoch:30/40, step:10/13, loss=0.34627529978752136, avg_loss=0.3509579598903656\n",
      "epoch:30/40, all_loss=4.551311492919922\n",
      "epoch:31/40, step:0/13, loss=0.3436848819255829, avg_loss=0.3436848819255829\n",
      "epoch:31/40, step:5/13, loss=0.3413189649581909, avg_loss=0.34217900037765503\n",
      "epoch:31/40, step:10/13, loss=0.33666178584098816, avg_loss=0.340436190366745\n",
      "epoch:31/40, all_loss=4.416213035583496\n",
      "epoch:32/40, step:0/13, loss=0.3317122459411621, avg_loss=0.3317122459411621\n",
      "epoch:32/40, step:5/13, loss=0.33024853467941284, avg_loss=0.33117160201072693\n",
      "epoch:32/40, step:10/13, loss=0.325892835855484, avg_loss=0.32948869466781616\n",
      "epoch:32/40, all_loss=4.27821683883667\n",
      "epoch:33/40, step:0/13, loss=0.32548654079437256, avg_loss=0.32548654079437256\n",
      "epoch:33/40, step:5/13, loss=0.322843462228775, avg_loss=0.32189157605171204\n",
      "epoch:33/40, step:10/13, loss=0.31681737303733826, avg_loss=0.32084041833877563\n",
      "epoch:33/40, all_loss=4.162047386169434\n",
      "epoch:34/40, step:0/13, loss=0.31625810265541077, avg_loss=0.31625810265541077\n",
      "epoch:34/40, step:5/13, loss=0.3106943964958191, avg_loss=0.3131254315376282\n",
      "epoch:34/40, step:10/13, loss=0.3097839057445526, avg_loss=0.3122239112854004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/250 [00:00<00:06, 36.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34/40, all_loss=4.0526909828186035\n",
      "testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:06<00:00, 40.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrr: 0.4235999882221222, hit@1: 0.4236, hit@3: 0.6152, hit@10: 0.7704  \n",
      "epoch:35/40, step:0/13, loss=0.3064205050468445, avg_loss=0.3064205050468445\n",
      "epoch:35/40, step:5/13, loss=0.3014841079711914, avg_loss=0.3046500086784363\n",
      "epoch:35/40, step:10/13, loss=0.30210477113723755, avg_loss=0.30372706055641174\n",
      "epoch:35/40, all_loss=3.9455783367156982\n",
      "epoch:36/40, step:0/13, loss=0.29986220598220825, avg_loss=0.29986220598220825\n",
      "epoch:36/40, step:5/13, loss=0.2972918450832367, avg_loss=0.2990383207798004\n",
      "epoch:36/40, step:10/13, loss=0.2935571074485779, avg_loss=0.2972831428050995\n",
      "epoch:36/40, all_loss=3.8579840660095215\n",
      "epoch:37/40, step:0/13, loss=0.29156604409217834, avg_loss=0.29156604409217834\n",
      "epoch:37/40, step:5/13, loss=0.2910342216491699, avg_loss=0.2913801372051239\n",
      "epoch:37/40, step:10/13, loss=0.2894535958766937, avg_loss=0.2902997136116028\n",
      "epoch:37/40, all_loss=3.7649052143096924\n",
      "epoch:38/40, step:0/13, loss=0.2863129675388336, avg_loss=0.2863129675388336\n",
      "epoch:38/40, step:5/13, loss=0.2846433222293854, avg_loss=0.28457409143447876\n",
      "epoch:38/40, step:10/13, loss=0.28090938925743103, avg_loss=0.2836560010910034\n",
      "epoch:38/40, all_loss=3.683689832687378\n",
      "epoch:39/40, step:0/13, loss=0.2787289321422577, avg_loss=0.2787289321422577\n",
      "epoch:39/40, step:5/13, loss=0.27567365765571594, avg_loss=0.2773953378200531\n",
      "epoch:39/40, step:10/13, loss=0.27857550978660583, avg_loss=0.2771150767803192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5/250 [00:00<00:06, 40.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39/40, all_loss=3.5971519947052\n",
      "testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:06<00:00, 40.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrr: 0.4203999936580658, hit@1: 0.4204, hit@3: 0.6144, hit@10: 0.7728  \n"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "model = TransE(len(ent2id), len(rel2id), norm=distance_norm, dim=embedding_dim).cuda()\n",
    "# model.load_state_dict(torch.load('transE_best.pth'))\n",
    "# 优化器adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# 损失函数， 对于本例，loss=max(0, (pd-nd)+1)， 负样本距离越小，正样本距离越大越好\n",
    "criterion = nn.MarginRankingLoss(margin=margin, reduction='mean')\n",
    "# 开始训练\n",
    "print('start training...')\n",
    "for epoch in range(epochs):\n",
    "    all_loss = 0\n",
    "    for i, (local_heads, local_relations, local_tails) in enumerate(train_data_loader):\n",
    "\n",
    "        positive_triples = torch.stack((local_heads, local_relations, local_tails), dim=1).cuda()\n",
    "\n",
    "        # 生成负样本\n",
    "        head_or_tail = torch.randint(high=2, size=local_heads.size())\n",
    "        random_entities = torch.randint(high=len(ent2id), size=local_heads.size())\n",
    "        broken_heads = torch.where(head_or_tail == 1, random_entities, local_heads)\n",
    "        broken_tails = torch.where(head_or_tail == 0, random_entities, local_tails)\n",
    "        negative_triples = torch.stack((broken_heads, local_relations, broken_tails), dim=1).cuda()\n",
    "\n",
    "        # # 生成负样本, 只打乱tail\n",
    "        # random_entities = torch.randint(high=len(ent2id), size=local_heads.size())\n",
    "        # negative_triples = torch.stack((random_entities, local_relations, random_entities), dim=1).cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pd, nd = model(positive_triples, negative_triples)\n",
    "        # pd要尽可能小， nd要尽可能大\n",
    "        loss = criterion(pd, nd, torch.tensor([-1], dtype=torch.long).cuda())\n",
    "        loss.backward()\n",
    "        all_loss += loss.data\n",
    "        optimizer.step()\n",
    "        if i % print_frequency == 0:\n",
    "            print(\n",
    "                f\"epoch:{epoch}/{epochs}, step:{i}/{len(train_data_loader)}, loss={loss.item()}, avg_loss={all_loss / (i + 1)}\")\n",
    "    print(f\"epoch:{epoch}/{epochs}, all_loss={all_loss}\")\n",
    "\n",
    "    # 验证\n",
    "    if validation and (epoch + 1) % dev_interval == 0:\n",
    "        print('testing...')\n",
    "        improve = ''\n",
    "        mrr, hits1, hits3, hits10 = model.evaluate(dev_data_loader)\n",
    "        if mrr >= best_mrr:\n",
    "            best_mrr = mrr\n",
    "            improve = '*'\n",
    "            torch.save(model.state_dict(), 'transE_best.pth')\n",
    "        torch.save(model.state_dict(), 'transE_latest.pth')\n",
    "        print(f'mrr: {mrr}, hit@1: {hits1}, hit@3: {hits3}, hit@10: {hits10}  {improve}')\n",
    "    if not validation:\n",
    "        torch.save(model.state_dict(), 'transE_latest.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:05<00:00, 42.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction finished !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict_all = []\n",
    "model.load_state_dict(torch.load('transE_best.pth'))\n",
    "for heads, relations in tqdm.tqdm(test_data_loader):\n",
    "    # 预测的id,结果为tensor(batch_size*10)\n",
    "    predict_id = model.link_predict(heads.cuda(), relations.cuda())\n",
    "    # 结果取到cpu并转为一行的list以便迭代\n",
    "    predict_list = predict_id.cpu().numpy().reshape(1,-1).squeeze(0).tolist()\n",
    "    # id转为实体\n",
    "    predict_ent = map(lambda x: id2ent[x], predict_list)\n",
    "    # 保存结果\n",
    "    predict_all.extend(predict_ent)\n",
    "print('prediction finished !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 写入文件并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved !\n"
     ]
    }
   ],
   "source": [
    "# 写入文件，按提交要求\n",
    "with open('submission.tsv', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(test)):\n",
    "        # 直接writelines没有空格分隔，手工加分割符，得按提交格式来\n",
    "        list = [x + '\\t' for x in test[i]] + [x + '\\n' if i == 9 else x + '\\t' for i, x in enumerate(predict_all[i*10:i*10+10])]\n",
    "        f.writelines(list)\n",
    "print('file saved !')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [
    {
     "id": "137349",
     "title": "获取数据集标题失败"
    }
   ],
   "description": "",
   "notebookId": "420547",
   "source": "dsw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
